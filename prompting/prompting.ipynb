{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba99df3-7031-44e6-9c2f-bbffcf48fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691c730e-d4f3-4b4c-bb1d-9845fa21d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a738cc-31da-446f-bf57-05eefc5a7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ede3e5-7e25-40f5-b93f-20ac2434beff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really, really smart puppy. This puppy can learn from things you show it and things it sees.\n",
       "\n",
       "**AI is like that puppy, but instead of being a real animal, it's a program on a computer!**\n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "*   **Learning:** Just like your puppy learns \"sit\" or \"fetch,\" AI learns from tons and tons of information called \"data.\" For example, an AI could learn to recognize pictures of cats by seeing millions of cat pictures!\n",
       "*   **Making Choices:** After learning, the AI can make choices based on what it has learned. If you show it a new picture, it can tell you if it thinks it's a cat or not.\n",
       "*   **Getting Better:** The more data the AI sees and the more choices it makes, the better it gets at making correct choices. So, it's like your puppy getting better at fetching the more you play with it!\n",
       "\n",
       "**Where do you see AI?**\n",
       "\n",
       "*   **Siri or Alexa:** When you ask them questions, they're using AI to understand you and find the answers!\n",
       "*   **Video Games:** Sometimes, the characters you play against are controlled by AI!\n",
       "*   **Netflix:** When Netflix recommends shows you might like, it's using AI to guess what you'll enjoy!\n",
       "*   **Self-driving cars:** These use AI to navigate and drive without a human!\n",
       "\n",
       "**So, AI is basically a super-smart computer program that can learn from data and make choices, just like a puppy learning tricks!**\n",
       "\n",
       "It's used everywhere to make things easier and more fun! Just remember, it's not magic, it's just really clever programming.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain AI to me like I'm a kid.\")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed13334f-8f9f-4b04-988a-4f9a75b3de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Zlork! It's nice to meet you. I'm a large language model. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('Hello! My name is Zlork.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f063f87b-6be3-4cb8-9f1b-d2042fa751a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, your name is Zlork.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7253178-eac6-48d3-abf0-b3f346650fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7f24c3-52f2-450c-af4e-12d941ca2609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent', 'countTokens'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87915af1-bdd3-4197-bf68-84eab91c48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_config = types.GenerateContentConfig(max_output_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6bd2e27-a8a0-4a96-be76-f2b4667397fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentConfig(http_options=None, system_instruction=None, temperature=None, top_p=None, top_k=None, candidate_count=None, max_output_tokens=200, stop_sequences=None, response_logprobs=None, logprobs=None, presence_penalty=None, frequency_penalty=None, seed=None, response_mime_type=None, response_schema=None, routing_config=None, safety_settings=None, tools=None, tool_config=None, labels=None, cached_content=None, response_modalities=None, media_resolution=None, speech_config=None, audio_timestamp=None, automatic_function_calling=None, thinking_config=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831bc16-1e70-4c04-9ff6-6b20022250fb",
   "metadata": {},
   "source": [
    "### Thinking Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be25de0-9d3f-400f-9e7b-a3c9e90a3f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The youngest author listed on the original Transformer paper, **\"Attention is All You Need\"**, is likely **Aidan N. Gomez**.\n",
       "\n",
       "Here's why:\n",
       "\n",
       "* **\"Attention is All You Need\"** is the seminal paper that introduced the Transformer architecture.\n",
       "* The authors are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.\n",
       "* **Aidan N. Gomez** was a PhD student at the University of Toronto at the time of the paper's publication in 2017.  PhD students are generally younger than established researchers. The other authors were mostly researchers at Google Brain or other established institutions.\n",
       "\n",
       "While we don't have publicly available birthdates for all authors to definitively confirm the absolute youngest,  being a PhD student during the publication of such a groundbreaking paper strongly suggests Aidan N. Gomez was the youngest author in the team.\n",
       "\n",
       "Therefore, based on typical academic career stages and publicly available information, **Aidan N. Gomez** is highly likely to be the youngest author."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a5be8-4b5f-4d74-bffd-81d865c25e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
